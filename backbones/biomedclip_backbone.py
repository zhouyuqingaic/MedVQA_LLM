# -*- coding: utf-8 -*-
"""
BiomedCLIP BackBoneÔºàÈáçÊûÑÁâàÔºåÊó†È¢ùÂ§ñÂΩí‰∏ÄÂåñÔºâ
==========================================
ÁâπÁÇπÔºö
- Âè™ÊîØÊåÅ‚ÄúÊú¨Âú∞ÁõÆÂΩïÂä†ËΩΩ‚ÄùÔºöÁõÆÂΩï‰∏≠ÂøÖÈ°ªÂåÖÂê´
    - open_clip_config.json
    - open_clip_pytorch_model.bin / open_clip_pytorch_model.ptÔºà‰∫åÈÄâ‰∏ÄÔºâ
- ‰ΩøÁî® open_clip ÂÆòÊñπÊé®ËçêÊñπÂºèÊûÑÂª∫Ê®°ÂûãÔºàÂèÇËÄÉ HuggingFace README ÁöÑ 2.2 Êú¨Âú∞Âä†ËΩΩÁ§∫‰æãÔºâ
- ‰∏çÂÜçÊâãÂä®ËÆøÈóÆ visual.conv1 / conv1_1ÔºåÈÅøÂÖç TimmModel ÂëΩÂêçÂ∑ÆÂºÇÂØºËá¥ÁöÑ AttributeError
- **‰∏çÊ∑ªÂä†‰ªª‰ΩïÈ¢ùÂ§ñ LayerNorm / ÂΩí‰∏ÄÂåñ**Ôºåencode_image / encode_text ÁöÑËæìÂá∫Âç≥‰∏∫ BiomedCLIP ÂéüÁîüËæìÂá∫
- Êö¥Èú≤Ôºö
    - preprocess_image()    : PIL / ndarray -> tensor
    - tokenize()            : list[str] -> input_ids / attention_mask
    - encode_image()        : pixel_values -> ÂõæÂÉèÂÖ®Â±ÄÁâπÂæÅ
    - encode_text()         : input_ids -> ÊñáÊú¨ÂÖ®Â±ÄÁâπÂæÅ
    - forward()             : ÂêåÊó∂ÁºñÁ†ÅÂõæÂÉè + ÊñáÊú¨ÔºåËøîÂõû dict
"""

import os
import json
from typing import Dict

import torch
import torch.nn as nn
import open_clip
from open_clip.factory import _MODEL_CONFIGS  # Ê≥®ÂÜåÊú¨Âú∞Ê®°ÂûãÈÖçÁΩÆ


class BiomedCLIPBackbone(nn.Module):
    """
    Áªü‰∏ÄÁöÑ BiomedCLIP backbone Â∞ÅË£ÖÔºö
    - Âè™ÂÅö‚ÄúÂÖ®Â±ÄÂõæÂÉè / ÊñáÊú¨ÁâπÂæÅ‚ÄùÁöÑÊäΩÂèñÔºå‰∏çÂÜçÊâãÂÜô ViT ÁªÜËäÇ
    - ÈÄöËøá config ‰∏≠ÁöÑ embed_dim Ëá™Âä®Á°ÆÂÆöËæìÂá∫Áª¥Â∫¶
    - ‰∏çÂú®ËæìÂá∫‰∏äÂè†Âä†‰ªª‰ΩïÈ¢ùÂ§ñÂΩí‰∏ÄÂåñÂ±ÇÔºåÂÆåÂÖ®Ê≤øÁî® BiomedCLIP ÂéüÁîüËæìÂá∫
    """

    def __init__(
        self,
        model_dir: str,
        device: str = "cuda",
        context_length: int = 256,
        freeze_vision: bool = False,
        freeze_text: bool = False,
    ) -> None:
        super().__init__()

        # --------- Âü∫Êú¨Â±ûÊÄß ---------
        self.model_dir = str(model_dir)
        self.device = torch.device(device)
        self.context_length = int(context_length)

        if not os.path.isdir(self.model_dir):
            raise RuntimeError(
                f"[BiomedCLIPBackbone] Âè™ÊîØÊåÅ‚ÄúÊú¨Âú∞ÁõÆÂΩïÂä†ËΩΩ‚ÄùÔºåËØ∑‰º†ÂÖ•ÂåÖÂê´ÊùÉÈáç‰∏éÈÖçÁΩÆÊñá‰ª∂ÁöÑÁõÆÂΩïÔºö{self.model_dir}"
            )

        # --------- 1. Ê£ÄÊü•ÂøÖÈ°ªÊñá‰ª∂ ---------
        cfg_path = os.path.join(self.model_dir, "open_clip_config.json")
        bin_path = os.path.join(self.model_dir, "open_clip_pytorch_model.bin")
        pt_path = os.path.join(self.model_dir, "open_clip_pytorch_model.pt")

        if not os.path.isfile(cfg_path):
            raise FileNotFoundError(f"[BiomedCLIPBackbone] Áº∫Â∞ëÈÖçÁΩÆÊñá‰ª∂Ôºö{cfg_path}")
        if not (os.path.isfile(bin_path) or os.path.isfile(pt_path)):
            raise FileNotFoundError(
                "[BiomedCLIPBackbone] Áº∫Â∞ëÊùÉÈáçÊñá‰ª∂Ôºö"
                "open_clip_pytorch_model.bin / open_clip_pytorch_model.pt Ëá≥Â∞ëÂ≠òÂú®‰∏Ä‰∏™„ÄÇ"
            )

        # --------- 2. ËØªÂèñ config Âπ∂Ê≥®ÂÜåÂà∞ open_clip ÁöÑÊú¨Âú∞Ê®°ÂûãË°® ---------
        with open(cfg_path, "r", encoding="utf-8") as f:
            config_json = json.load(f)

        model_cfg = config_json["model_cfg"]
        preprocess_cfg = config_json["preprocess_cfg"]

        # Ëøô‰∏™ÂêçÂ≠óÂèØ‰ª•Èöè‰æøÂèñÔºåÂè™Ë¶ÅÂú® _MODEL_CONFIGS ÈáåÊ≥®ÂÜåÂ∞±Ë°å
        local_model_name = "biomedclip_local_refactored"
        if local_model_name not in _MODEL_CONFIGS:
            _MODEL_CONFIGS[local_model_name] = model_cfg

        # ÈÄâÊã©ÂÆûÈôÖ‰ΩøÁî®ÁöÑÊùÉÈáçÊñá‰ª∂
        weight_file = bin_path if os.path.isfile(bin_path) else pt_path

        # Êää preprocess ÁöÑ image_* ÂèÇÊï∞‰∫§Áªô open_clip Êù•ÊûÑÂª∫ transform
        image_kwargs = {f"image_{k}": v for k, v in preprocess_cfg.items()}

        print(f"[BiomedCLIPBackbone] Loading BiomedCLIP from: {self.model_dir}")
        print(f"[BiomedCLIPBackbone] Using weight file: {weight_file}")
        print(f"[BiomedCLIPBackbone] Target device: {self.device}")

        # --------- 3. Ë∞ÉÁî® open_clip ÂàõÂª∫ÂÆåÊï¥ CLIP Ê®°Âûã + È¢ÑÂ§ÑÁêÜ ---------
        # ÂØπÈΩêÂÆòÊñπ README ÁöÑÊú¨Âú∞Âä†ËΩΩÊñπÂºèÔºàÂè™ÊòØÊääË∑ØÂæÑÊîπ‰∏∫‰Ω†Ëá™Â∑±ÁöÑÁõÆÂΩïÔºâ
        self.clip, _, self._image_preprocess = open_clip.create_model_and_transforms(
            model_name=local_model_name,
            pretrained=weight_file,
            device=self.device,
            **image_kwargs,
        )
        self._tokenizer = open_clip.get_tokenizer(local_model_name)

        # clip Â∑≤ÁªèÂú® device ‰∏äÔºåËøôÈáå‰∏çÁî®ÂÜçÊâãÂä® .to(self.device)

        # --------- 4. Áªü‰∏ÄÊö¥Èú≤ÁöÑÊé•Âè£ÔºàÂØπ‰∏äÂ±ÇÂèãÂ•ΩÔºâ ---------
        # ‰∏é‰Ω†‰πãÂâçÂèØËøêË°åÁâàÊú¨‰øùÊåÅ‰∏ÄËá¥ÁöÑÂ±ûÊÄßÂëΩÂêç
        self.preprocess = self._image_preprocess
        self.preprocess_val = self._image_preprocess
        self.tokenizer = self._tokenizer

        # ‰ªéÈÖçÁΩÆ‰∏≠ËØªÂèñ embed_dimÔºåÈÅøÂÖç‰æùËµñÂÜÖÈÉ®ÂëΩÂêçÔºàÊØîÂ¶Ç visual.head.out_features Á≠âÔºâ
        embed_dim = int(model_cfg.get("embed_dim", 512))
        self.img_dim = embed_dim
        self.txt_dim = embed_dim

        # üëâ ‰∏çÂÅö‰ªª‰ΩïÈ¢ùÂ§ñÂΩí‰∏ÄÂåñÔºå‰øùÊåÅ BiomedCLIP ÂéüÁîüËæìÂá∫
        # self.norm_img = nn.Identity()
        # self.norm_txt = nn.Identity()

        # --------- 5. ÊòØÂê¶ÂÜªÁªìËßÜËßâ / ÊñáÊú¨ÁºñÁ†ÅÂô® ---------
        if freeze_vision:
            for p in self.clip.visual.parameters():
                p.requires_grad = False
            print("[BiomedCLIPBackbone] Vision encoder is FROZEN.")
        else:
            print("[BiomedCLIPBackbone] Vision encoder is TRAINABLE.")

        # if freeze_text:
        #     for p in self.clip.transformer.parameters():
        #         p.requires_grad = False
        #     print("[BiomedCLIPBackbone] Text encoder is FROZEN.")
        # else:
        #     print("[BiomedCLIPBackbone] Text encoder is TRAINABLE.")

        # ‰øÆÊîπÂêéÁöÑ‰ª£Á†ÅÔºöËá™Âä®Ê£ÄÊµãÂ±ûÊÄßÂêç
        if freeze_text:
            # Â∞ùËØïÊü•ÊâæÂ∏∏ËßÅÁöÑÊñáÊú¨ÁºñÁ†ÅÂô®Â±ûÊÄßÂêç
            if hasattr(self.clip, "text"):
                # BiomedCLIP / CustomTextCLIP ÈÄöÂ∏∏Ëµ∞ËøôÈáå
                text_encoder = self.clip.text
            elif hasattr(self.clip, "transformer"):
                # Ê†áÂáÜ CLIP ÈÄöÂ∏∏Ëµ∞ËøôÈáå
                text_encoder = self.clip.transformer
            elif hasattr(self.clip, "bert"):
                text_encoder = self.clip.bert
            else:
                print(
                    f"Warning: [{self.__class__.__name__}] Could not find text encoder to freeze (no .text or .transformer).")
                text_encoder = None

            if text_encoder is not None:
                for p in text_encoder.parameters():
                    p.requires_grad = False
                print(f"[{self.__class__.__name__}] Text encoder frozen.")

        print(
            f"[BiomedCLIPBackbone] Load success. "
            f"img_dim={self.img_dim}, txt_dim={self.txt_dim}, context_length={self.context_length}"
        )

    # ------------------------------------------------------------------
    #   ÁºñÁ†ÅÂáΩÊï∞ÔºöÂØπ‰∏äÂ±ÇÊö¥Èú≤ÁöÑ‚ÄúÂπ≤ÂáÄÊé•Âè£‚Äù
    # ------------------------------------------------------------------
    @torch.no_grad()
    def encode_image(self, pixel_values: torch.Tensor) -> torch.Tensor:
        """
        ÂõæÂÉèÁºñÁ†ÅÔºàÂéüÁîü BiomedCLIP ËæìÂá∫ÔºâÔºö
        - ËæìÂÖ•Ôºöpixel_values: [B, 3, H, W]ÔºåÈúÄÂÖàÁªèËøá preprocess (CLIP mean/std)
        - ËæìÂá∫Ôºöimg_feat: [B, img_dim]
        """
        pixel_values = pixel_values.to(self.device, non_blocking=True)
        img_feat = self.clip.encode_image(pixel_values)  # open_clip Â∑≤ÁªèÂ§ÑÁêÜÂ•ΩËßÜËßâÂ°î & ÊäïÂΩ±
        # ‰∏çÂÅöÈ¢ùÂ§ñ LayerNorm / ÂΩí‰∏ÄÂåñÔºåÁõ¥Êé•ËøîÂõû BiomedCLIP ÂéüÂßã embedding
        return img_feat

    @torch.no_grad()
    def encode_text(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor | None = None,
    ) -> torch.Tensor:
        """
        ÊñáÊú¨ÁºñÁ†ÅÔºàÂéüÁîü BiomedCLIP ËæìÂá∫ÔºâÔºö
        - ËæìÂÖ•Ôºöinput_ids: [B, L]ÔºåÂª∫ËÆÆÁî± self.tokenize() ÁîüÊàê
        - ËæìÂá∫Ôºötxt_feat: [B, txt_dim]
        """
        # open_clip ÁöÑ text encoder ‰∏ç‰ΩøÁî® attention_maskÔºåËøôÈáå‰øùÊåÅÊé•Âè£‰∏ÄËá¥Âç≥ÂèØ
        input_ids = input_ids.to(self.device, non_blocking=True)
        txt_feat = self.clip.encode_text(input_ids)
        # ‰∏çÂÅöÈ¢ùÂ§ñ LayerNorm / ÂΩí‰∏ÄÂåñ
        return txt_feat

    # ------------------------------------------------------------------
    #   forwardÔºöÂêåÊó∂ÁºñÁ†Å image + textÔºåÊñπ‰æø‰∏äÂ±ÇÁõ¥Êé•Ë∞ÉÁî®
    # ------------------------------------------------------------------
    def forward(
        self,
        pixel_values: torch.Tensor,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor | None = None,
    ) -> Dict[str, torch.Tensor]:
        """
        ÈªòËÆ§ÂâçÂêëÔºö
        - ÂêåÊó∂ËæìÂá∫ÂõæÂÉè & ÊñáÊú¨ÁöÑÂÖ®Â±ÄÁâπÂæÅÔºå‰ª•Âèä‰∏Ä‰∏™ÁÆÄÂçïÁöÑÊãºÊé•ÂÖ®Â±ÄÂêëÈáè

        ËøîÂõûÔºö
            {
                "img_feat":   [B, img_dim],
                "txt_feat":   [B, txt_dim],
                "global_feat":[B, img_dim + txt_dim]
            }
        """
        img_feat = self.encode_image(pixel_values)
        txt_feat = self.encode_text(input_ids, attention_mask)

        # ‰Ω†‰πãÂâçÂ∞±ÊòØÁÆÄÂçïÊãºÊé•Ôºå‰øùÊåÅË°å‰∏∫‰∏çÂèò
        global_feat = torch.cat([img_feat, txt_feat], dim=-1)
        return {
            "img_feat": img_feat,
            "txt_feat": txt_feat,
            "global_feat": global_feat,
        }

    # ------------------------------------------------------------------
    #   ÂÆûÁî®Â∑•ÂÖ∑ÂáΩÊï∞ÔºöË∑ü‰πãÂâçÂèØËøêË°åÁâàÊú¨‰øùÊåÅ‰∏ÄËá¥
    # ------------------------------------------------------------------
    def preprocess_image(self, pil_or_ndarray) -> torch.Tensor:
        """
        ÂØπÂçïÂº†ÂõæÁâáÂÅö BiomedCLIP Ê†áÂáÜÈ¢ÑÂ§ÑÁêÜÔºåËøîÂõû [3, H, W] ÁöÑ tensor„ÄÇ
        """
        return self._image_preprocess(pil_or_ndarray)

    def tokenize(self, texts: list[str]) -> Dict[str, torch.Tensor]:
        """
        Êää‰∏Ä‰∏™ batch ÁöÑÊñáÊú¨ËΩ¨Êàê input_ids / attention_mask„ÄÇ
        - Ê≥®ÊÑèÔºöopen_clip ÁöÑ tokenizer ËøîÂõûÁöÑÊòØ tensorÔºåËÄå‰∏çÊòØ HF ÁöÑ BatchEncoding„ÄÇ
        """
        ids = self._tokenizer(texts, context_length=self.context_length)
        attn = torch.ones_like(ids)
        return {"input_ids": ids, "attention_mask": attn}


# ======================================================================
#                              Ëá™ÊµãËÑöÊú¨
# ======================================================================
if __name__ == "__main__":
    """
    ÁÆÄÂçïËá™ÊµãÔºö
    1. ‰ªéÊú¨Âú∞ÁõÆÂΩïÂä†ËΩΩ BiomedCLIP
    2. Áî®ÈöèÊú∫ tensor Ê®°Êãü‰∏ÄÊâπÂõæÂÉè + ÊñáÊú¨ id
    3. ÊâìÂç∞ËæìÂá∫ÁöÑ shapeÔºåÊ£ÄÊü•ÊòØÂê¶Á¨¶ÂêàÈ¢ÑÊúü
    """

    # TODO: Êåâ‰Ω†Ëá™Â∑±ÁöÑË∑ØÂæÑ‰øÆÊîπ
    LOCAL_DIR = "/home/yuqing/Models/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224"

    device = "cuda:4" if torch.cuda.is_available() else "cpu"
    print(f"[TEST] Running BiomedCLIPBackbone test on device: {device}")

    backbone = BiomedCLIPBackbone(
        model_dir=LOCAL_DIR,
        device=device,
        context_length=256,
        freeze_vision=False,
        freeze_text=False,
    )

    # ---- ÊûÑÈÄ†ÂÅáÊï∞ÊçÆÔºö2 Âº†ÈöèÊú∫ÂõæÂÉè + 2 Êù°‰º™ÊñáÊú¨ ----
    B = 2
    dummy_img = torch.randn(B, 3, 224, 224)  # Âè™ÊòØ shape Ê£ÄÊü•Áî®
    dummy_texts = ["this is a dummy sentence", "another dummy text"]

    # tokenizer -> input_ids / attention_mask
    tok_out = backbone.tokenize(dummy_texts)
    input_ids = tok_out["input_ids"]
    attention_mask = tok_out["attention_mask"]

    # ÊääÊï∞ÊçÆ‰∏¢Âà∞Âêå‰∏Ä device
    dummy_img = dummy_img.to(device)
    input_ids = input_ids.to(device)
    attention_mask = attention_mask.to(device)

    with torch.no_grad():
        out = backbone(
            pixel_values=dummy_img,
            input_ids=input_ids,
            attention_mask=attention_mask,
        )

    print("\n[TEST] Output shapes:")
    print(f"  img_feat   : {tuple(out['img_feat'].shape)}")     # [B, img_dim]
    print(f"  txt_feat   : {tuple(out['txt_feat'].shape)}")     # [B, txt_dim]
    print(f"  global_feat: {tuple(out['global_feat'].shape)}") # [B, img_dim + txt_dim]

    print("\n[TEST] Done. BiomedCLIPBackbone is working.")

    #Ëá™ÊµãËæìÂá∫
    """
    /opt/anaconda3/condabin/conda run -n MoEBiomedVQA_LLM --no-capture-output python /home/yuqing/RemoteProjects/MedVQA_LLM_P02/backbones/biomedclip_backbone.py 
    /home/yuqing/.conda/envs/MoEBiomedVQA_LLM/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
      warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
    [TEST] Running BiomedCLIPBackbone test on device: cuda:4
    [BiomedCLIPBackbone] Loading BiomedCLIP from: /home/yuqing/Models/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224
    [BiomedCLIPBackbone] Using weight file: /home/yuqing/Models/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224/open_clip_pytorch_model.bin
    [BiomedCLIPBackbone] Target device: cuda:4
    [BiomedCLIPBackbone] Vision encoder is TRAINABLE.
    [BiomedCLIPBackbone] Text encoder is TRAINABLE.
    [BiomedCLIPBackbone] Load success. img_dim=512, txt_dim=512, context_length=256

    [TEST] Output shapes:
      img_feat   : (2, 512)
      txt_feat   : (2, 512)
      global_feat: (2, 1024)

    [TEST] Done. BiomedCLIPBackbone is working.

    Process finished with exit code 0 
    """
