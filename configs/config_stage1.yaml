# configs/config_stage1.yaml
# Tokens Route (Full Patch Tokens) - Stage 1 Alignment

# ----------------------------
# Runtime
# ----------------------------
gpus: "4,5"

# ----------------------------
# Model
# ----------------------------
model_path: "/home/yuqing/Models/Qwen2.5-7B-Instruct"

# ----------------------------
# Data (LLaVA-Med alignment)
# ----------------------------
llava_med_json: "/home/yuqing/Datas/llava_med_alignment_500k_subset.json"
llava_med_image_feature_dir: "/home/yuqing/Datas/llava_med_images_biomedclip_pt"

# 离线特征：patch tokens [T, 768]（例如 T=197）
image_token_dim: 768

# 可选 system prompt（Stage-1 推荐：短、稳、对齐 LLaVA 风格）
#Stage‑1 对齐阶段：优先用 方案 A。因为 alignment 数据很多时候是“描述/问答/解释”风格，
# system 太“安全化”可能会让模型在训练时学到“动不动加免责声明”的偏好（跟监督答案分布不一致）。
system_prompt: |
  A chat between a curious user and an AI medical assistant.
  The assistant gives helpful, detailed, and polite answers about biomedical images and medical topics.
  Ground the answer in the provided image and the user's question; do not invent findings or measurements.
  If the question cannot be answered from the given information, say you are unsure and briefly explain what is missing.
  Keep the final answer concise unless the user asks for more detail.


# ----------------------------
# Output
# ----------------------------
output_dir: "../med_stage1_align_output"

# ----------------------------
# LoRA / QLoRA
# ----------------------------
lora:
  r: 64
  alpha: 128
  dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

# ----------------------------
# Vision wrapper (Tokens Route)
# ----------------------------
vision_adapter:
  type: "patch_prefix"        # ✅ 关键：LLaVA-style prefix 拼接（全 tokens）
  use_image_feat: true        # 保持字段名不变，表示启用图像分支
  prefix_dropout: 0.0

  # projector：逐 token 翻译 768 -> Qwen hidden(4096)
  projector_type: "linear"    # linear / mlp
  projector_hidden_dim: 768   # projector_type=mlp 时生效
  layer_norm: false           # projector 前 LN（可选）

# ----------------------------
# Training
# ----------------------------
micro_batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 2.0e-4
epochs: 1
max_length: 1024
num_workers: 8
bf16: true

# 调试用：限制样本数
# max_samples: 2000