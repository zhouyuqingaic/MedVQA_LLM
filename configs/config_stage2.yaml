# configs/config_stage2.yaml
#
# Stage-2 (VQA-RAD / PathVQA) config — tokens-route version
# ---------------------------------------------------------
# 目标：
# - 继续加载 Stage-1 checkpoint（LoRA + patch_prefix projector）
# - text: 纯文本训练（结构仍保持一致，便于加载 stage1；但不喂 image_tokens）
# - multi: 多模态训练（从离线 .pt tokens cache 查表，prefix 拼接）

gpus: "0,1,2,3"

# Stage-1 输出目录（Trainer 会从这里加载 model.safetensors / pytorch_model.bin）
stage1_ckpt_dir: "/path/to/med_stage1_align_output"

# 默认模态（可通过命令行 --modality 覆盖）
modality: "multi"   # "text" or "multi"

# --------------------------
# HF datasets cache paths
# --------------------------
vqa_rad_cache: "/path/to/hf_cache/vqa-rad"
path_vqa_cache: "/path/to/hf_cache/path-vqa"

# --------------------------
# Offline BiomedCLIP tokens cache (.pt)
# --------------------------
# 由脚本生成：
#   python med_vqa_datasets/gen_vqa_rad_path_pt.py --feature_type tokens --config configs/config_stage2.yaml
vqa_rad_pt_output: "/path/to/vqa_rad_biomedclip_tokens.pt"
path_vqa_pt_output: "/path/to/path_vqa_biomedclip_tokens.pt"

# （可选）如果你也想用 gen_vqa_rad_path_pt.py 抽特征，需要提供 BiomedCLIP 本地目录
# biomedclip_model_dir: "/path/to/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224"

# =========================================================
# Stage-2 text-only
# =========================================================
text:
  model_path: "/path/to/Qwen2.5-7B-Instruct"

  # tokens dim 必须与 .pt 里的 tokens 一致（ViT-B/16 通常 768）
  image_token_dim: 768

  # Vision adapter：仍然保持 patch_prefix 结构，以便加载 stage1 权重；
  # 但关闭 use_image_feat -> forward 自动走纯文本分支
  vision_adapter:
    type: "patch_prefix"
    use_image_feat: false
    prefix_dropout: 0.0
    projector_type: "linear"     # linear / mlp
    projector_hidden_dim: 768    # mlp 时可用
    layer_norm: false

  lora:
    r: 16
    alpha: 128
    dropout: 0.05
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"

  # ---- training ----
  output_dir: "/path/to/stage2_vqa_text_output"
  dataset_name: "vqa-rad"        # "vqa-rad" or "path-vqa"
  train_split: "train"
  eval_split: "test"
  cache_dir: null

  micro_batch_size: 4
  eval_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-4
  epochs: 3
  max_length: 512
  num_workers: 4
  bf16: true
  max_train_samples: null
  max_eval_samples: null
  seed: 42

  system_prompt: |
    You are a helpful medical AI assistant for visual question answering.
    In this training stage you will NOT see the image, only the text question.
    Please answer as accurately and concisely as possible.

# =========================================================
# Stage-2 multimodal (offline tokens)
# =========================================================
multi:
  model_path: "/path/to/Qwen2.5-7B-Instruct"
  image_token_dim: 768

  vision_adapter:
    type: "patch_prefix"
    use_image_feat: true
    prefix_dropout: 0.0
    projector_type: "linear"
    projector_hidden_dim: 768
    layer_norm: false

  lora:
    r: 16
    alpha: 128
    dropout: 0.05
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"

  output_dir: "/path/to/stage2_vqa_multi_output"
  dataset_name: "vqa-rad"
  train_split: "train"
  eval_split: "test"
  cache_dir: null

  micro_batch_size: 2
  eval_batch_size: 2
  gradient_accumulation_steps: 4
  learning_rate: 1.0e-4
  epochs: 3
  max_length: 512
  num_workers: 4
  bf16: true
  max_train_samples: null
  max_eval_samples: null
  seed: 42

  system_prompt: |
    You are a helpful medical AI assistant for visual question answering on radiology/pathology images.
    You will receive both the image (encoded into patch tokens) and the text question.
    Please provide concise and accurate answers.
