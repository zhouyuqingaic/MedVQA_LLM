# configs/config_stage2.yaml
#
# ç»Ÿä¸€çš„ Stage-2 é…ç½®æ–‡ä»¶ï¼š
# - é¡¶å±‚æœ‰å…¬å…±é…ç½®ï¼šgpusã€stage1_ckpt_dir
# - text: æ–‡æœ¬-only æ¨¡å¼ä¸“ç”¨é…ç½®
# - multi: å¤šæ¨¡æ€æ¨¡å¼ä¸“ç”¨é…ç½®
#
# æ³¨æ„ï¼š
#   1. text å’Œ multi ä¸¤å—ä¸‹é¢çš„å­—æ®µä¼šç›´æ¥ä¼ ç»™ engine.builder.build_vision_llmï¼Œ
#      å› æ­¤éœ€è¦åŒ…å«ï¼š
#          - model_path
#          - lora: { r, alpha, dropout, target_modules }
#          - vision_adapter: { enabled, type, ... }
#          - image_feat_dim
#   2. å¦‚æœä½ æƒ³å®Œå…¨å¤ç”¨ Stage-1 çš„ç»“æ„ï¼Œåªéœ€æŠŠ Stage-1 çš„è¿™äº›å­—æ®µå¤åˆ¶è¿‡æ¥å³å¯ã€‚
#   3. Stage-2 ä¼šä½¿ç”¨ stage1_ckpt_dir åŠ è½½ Stage-1 è®­ç»ƒå¥½çš„æƒé‡ã€‚

gpus: "0,1,2,3"    # æˆ– "0,1,2,3" ç­‰ï¼Œæ ¹æ®ä½ æœºå™¨æ”¹

# Stage-1 è¾“å‡ºç›®å½•ï¼ˆå°±æ˜¯ä½ ç»™å‡ºçš„é‚£ä¸ªè·¯å¾„ï¼‰
stage1_ckpt_dir: "/home/yuqing/RemoteProjects/MedVQA_LLM_P03/med_stage1_align_output"

# é»˜è®¤æ¨¡æ€ï¼ˆå¯è¢«å‘½ä»¤è¡Œ --modality è¦†ç›–ï¼‰
modality: "multi"   # "text" æˆ– "multi"

# ==========================
# æ•°æ®ç¼“å­˜è·¯å¾„ï¼ˆHF datasetsï¼‰
# ==========================
vqa_rad_cache: "/home/yuqing/Datas/vqa-rad/vqa-rad"
path_vqa_cache: "/home/yuqing/Datas/vqa-rad/path-vqa"
# ä½ åŸæ¥å·²æœ‰çš„å­—æ®µä¿æŒä¸å˜ï¼ˆä¾‹å¦‚ dataset_name, data_root ç­‰ï¼‰
# æ–°å¢ï¼š
vqa_rad_pt_output: "/home/yuqing/Datas/vqa-rad/vqa-rad/vqa_rad_biomedclip_imagefeat.pt"
path_vqa_pt_output: "/home/yuqing/Datas/vqa-rad/path-vqa/path_vqa_biomedclip_imagefeat.pt"



# ---------------------------
# æ–‡æœ¬-only Stage-2 é…ç½®
# ---------------------------
text:
  # === ä¸ Stage-1 æ„æ¨¡ç›¸å…³çš„éƒ¨åˆ†ï¼ˆè¯·ä¿æŒä¸€è‡´ï¼‰ ===
  model_path: "/home/yuqing/Models/Qwen2.5-7B-Instruct"
  image_feat_dim: 512

  lora:
    r: 16          # è¿™é‡Œä¹Ÿä¸€æ ·æ”¹æˆ 16
    alpha: 128
    dropout: 0.05
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"

  vision_adapter:
    enabled: true               # ä»ç„¶ä½¿ç”¨ Stage-1 çš„ Vision Adapter ç»“æ„
    type: "cross_attn"          # æˆ– "prefix" â€”â€” è¯·ä¸ Stage-1 ä¿æŒä¸€è‡´
    num_image_tokens: 4
    cross_attn_heads: 8
    cross_attn_dropout: 0.0
    use_gate: true
    use_image_feat: false       # ğŸŒŸ å…³é”®ï¼šæ–‡æœ¬-only æ¨¡å¼å…³é—­å›¾åƒåˆ†æ”¯ï¼ˆä½†ç»“æ„ä»æ˜¯å¤šæ¨¡æ€ï¼‰
    projector_type: "mlp"
    mlp_hidden_dim: null
    multihead_inner_dim: null
    moe_num_experts: 4
    moe_top_k: 2
    prefix_dropout: 0.0         # è‹¥ä½¿ç”¨ prefixï¼Œå¯åœ¨æ­¤å¤„è®¾ç½®

  # === Stage-2 è®­ç»ƒç›¸å…³è¶…å‚ ===
  output_dir: "/home/yuqing/RemoteProjects/MedVQA_LLM_P03/stage2_vqa_text_path_vqa" #æˆ–è€…stage2_vqa_text_vqa_rad,stage2_vqa_text_path_vqa
  dataset_name: "path-vqa"       # æˆ– "path-vqa","vqa-rad"
  train_split: "train"
  eval_split: "test"            # å¯¹äº path-vqa å¯ä»¥è®¾ç½®ä¸º "validation"
  cache_dir: null

  micro_batch_size: 4
  eval_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-4
  epochs: 3
  max_length: 512
  num_workers: 4
  bf16: true
  max_train_samples: null       # è°ƒè¯•æ—¶å¯ä»¥è®¾æˆ 512
  max_eval_samples: null
  seed: 42

  system_prompt: |
    You are a helpful medical AI assistant for visual question answering.
    In this training stage you will NOT see the image, only the text question.
    Please answer as accurately and concisely as possible.

# ---------------------------
# å¤šæ¨¡æ€ Stage-2 é…ç½®
# ---------------------------
multi:
  # === ä¸ Stage-1 æ„æ¨¡ç›¸å…³çš„éƒ¨åˆ†ï¼ˆè¯·ä¿æŒä¸€è‡´ï¼‰ ===
  model_path: "/home/yuqing/Models/Qwen2.5-7B-Instruct"
  image_feat_dim: 512

  lora:
    r: 16          # è¿™é‡Œä¹Ÿä¸€æ ·æ”¹æˆ 16
    alpha: 128
    dropout: 0.05
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"

  vision_adapter:
    enabled: true
    type: "cross_attn"

    # âœ… å¤šæ¨¡æ€é˜¶æ®µå¿…é¡»æ‰“å¼€å›¾åƒåˆ†æ”¯
    use_image_feat: true

    prefix_dropout: 0.0
    num_prefix_tokens: 64

    num_image_tokens: 4
    cross_attn_heads: 8
    cross_attn_dropout: 0.0
    use_gate: true

    projector_type: "multihead"
    mlp_hidden_dim: 512
    multihead_inner_dim: 512
    moe_num_experts: 4
    moe_top_k: 2

  # BiomedCLIP æ¨¡å‹ç›®å½•ï¼ˆéœ€æ›¿æ¢æˆä½ æœ¬åœ°çš„è·¯å¾„ï¼‰
  biomedclip_model_dir: "/home/yuqing/Models/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224"

  # === Stage-2 è®­ç»ƒç›¸å…³è¶…å‚ ===
  output_dir: "/home/yuqing/RemoteProjects/MedVQA_LLM_P03/stage2_vqa_multi_path_vqa" #æˆ–è€…:stage2_vqa_multi_vqa_rad,stage2_vqa_multi_path_vqa
  dataset_name: "path-vqa"       # æˆ– "path-vqa","vqa-rad"
  train_split: "train"
  eval_split: "test"
  cache_dir: null

  micro_batch_size: 2           # å¤šæ¨¡æ€ä¸€èˆ¬æ˜¾å­˜æ›´ç´§å¼ 
  eval_batch_size: 2
  gradient_accumulation_steps: 4
  learning_rate: 1.0e-4
  epochs: 3
  max_length: 512
  num_workers: 4
  bf16: true
  max_train_samples: null
  max_eval_samples: null
  seed: 42

  system_prompt: |
    You are a helpful medical AI assistant for visual question answering on radiology/pathology images.
    You will receive both the image (encoded by BiomedCLIP) and the text question.
    Please provide concise and accurate answers.
